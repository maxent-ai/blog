[
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#stakeholders-objectives-the-objective-interplay",
    "href": "posts/post-1/marketplace-recsys-part-1.html#stakeholders-objectives-the-objective-interplay",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Stakeholders, Objectives & the Objective interplay",
    "text": "Stakeholders, Objectives & the Objective interplay\nIn a multi-sided system with multiple objectives often the objectives interact and behave in different manners. At a high level, you could categorize the different types of these interplay as follows:\n\nCorrelated : Optimising for one objective helps the other.\nNeutral: Optimising for one does not impact the other\nAnti Correlated: Optimising for one hurts the other\n\nConsider the example of our Hypothetical on-demand delivery startup Hyper, which is a three-sided marketplace - i.e it has three sets of stakeholders with different motivations & objectives.\n\n\n\n\n\n\n\n\nStakeholder\nNeeds/Motivations\nPotential Objectives\n\n\n\n\nEnd User\nWants to order something from local partner shops.\n\nQuick delivery\nBest price\nReliable merchants\nFresh items\n\n\n\nMerchants\nProvide online visibility and find customers.\n\nMatching quality\nExposure\nMinimise wastage\n\n\n\nDelivery Partners\nEarn a stable livelihood.\n\nRegularity in jobs\nEarnings per partner\nEfficient drop location planning\n\n\n\n\nAs we see above each entity has a unique set of potential objectives one could optimise the recommendation system for & this has to be done in a deliberate and careful way or it might result in undesired outcomes.\n\n\n\n\n\n\nüßµ Superstar economics and towards a fair marketplace\n\n\n\n\n\nResearchers from Microsoft & Spotify found out that:\n\nRecommendation systems in general suffer from what is called as an inherent problem of ‚Äúsuperstar economics‚Äù: rankings have a top and a tail end, not just for popularity, but also for relevance. In an attempt to maximize user satisfaction, recommender system optimize for relevance. This inadvertently leads to lock-in of popular and relevant suppliers, especially for users who want to minimize the effort required to interact with the system. A major side-effect of the superstar economics is the impedance to suppliers on the tail-end of the spectrum, who struggle to attract consumers, given the low exposure, and thus, are not satisfied with the marketplace. Indeed, to continue to attract more suppliers to the platform, marketplaces face an interesting problem of optimizing their models for supplier exposure, and visibility. Indeed, the suppliers (e.g.¬†retailers, artists) would want a fair opportunity to be presented to the users.\n\nRead more about this in Towards a Fair Marketplace: Counterfactual Evaluation of the trade-off between Relevance, Fairness & Satisfaction in Recommendation Systems\n\n\n\nIn short, what a system optimises for is super important & in general, it needs to look for a balance while optimising for various objectives.\nBefore we proceed further on the track of optimising for multiple objectives (or multiple objective optimisation) - let us quickly visit how most modern recommendation systems look like & also do a quick recap of how one builds a model for ranking (aka Learning to rank) & some key metrics used for measuring these systems."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#candidate-retrieval",
    "href": "posts/post-1/marketplace-recsys-part-1.html#candidate-retrieval",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Candidate retrieval",
    "text": "Candidate retrieval\nThe Retrieval & filtering stages usually are collectively refered to as candidate generation, this is a fast but coarse step which essentially narrows down the search space of items from millions of candidates for a given query to soemthing in order of 100s.\nThis is often achieved by an initial retrieval with some form of matching between the query and the catalog usually via an ANN, Graph based approach or some form of decision trees. Followed by a filtering stage, where invalid candidates are removed from the initial retrieval before passing onto the next stages."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#ranking",
    "href": "posts/post-1/marketplace-recsys-part-1.html#ranking",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Ranking",
    "text": "Ranking\nAt this phase we further narrow down the initial set of items into a much smaller list that would be presented to the user. This is a slow but more precise operation - this stage is usually modelled as an LTR task or a classification task. Further, sometimes it‚Äôs followed by an ordering stage which handles various types of business logic to reorder/sort the final list of items. Some common examples of these business logic include: organising recommendations to fit genre distributions in streaming services or promoting certain segments of sellers as in case of ecommerce.\n\n\n\n\n\n\nüìù Suggested reading\n\n\n\nMost Production recommendation systems are indeed complex in nature, but the above pattern which was conceptulised at Nvidea by @Even_Oldridge & @karlhigley sort of provides a good overview of how these systems are designed & to read a bit more on this, refer to Recommender Systems not just recommender models article & also read system-design-for-discovery from @eugeneyan where he summarises this design pattern in depth and points to multiple examples from industry that follow this pattern."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#ranking-task",
    "href": "posts/post-1/marketplace-recsys-part-1.html#ranking-task",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Ranking Task",
    "text": "Ranking Task\nGiven a query \\(q\\), and a set of \\(n\\) documents \\(D=d_1,d_2,...,dn\\), we‚Äôd like to learn a function \\(f\\) such that \\(f(q, D)\\) will predict the relevance of any given document associated with a query."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#pointwise-method",
    "href": "posts/post-1/marketplace-recsys-part-1.html#pointwise-method",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Pointwise method",
    "text": "Pointwise method\nIn pointwise method, the above ranking task is re-formulated as a standard classification/regression task. The function to be learned \\(f(q, D)\\) is simplified as \\(f(q, d_i)\\) i.e the relevance of each document given a query is scored independently.\nFor instance, if we have two queries associated with 2 and 3 resulting matched documents:\n\n\\(q_1 \\to d_1, d_2\\)\n\\(q_2 \\to d_3, d_4, d_5\\)\n\nThen the training data \\(x_i\\) in a pointwise method will essentially be every query-document pair as follows:\n\n\\(x_1 : q_1, d_1\\)\n\\(x_2 : q_1, d_2\\)\n\\(x_3 : q_2, d_3\\)\n\\(x_4 : q_2, d_4\\)\n\\(x_5 : q_2, d_5\\)\n\nSince each document is scored independently with the absolute relevance as the target label, the task is no different from any standard classification or regression task. As such any standard ml algorithms can be leveraged in this setting."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#pairwise-method",
    "href": "posts/post-1/marketplace-recsys-part-1.html#pairwise-method",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Pairwise method",
    "text": "Pairwise method\nIn pairwise method, the goal is to learn a pointwise scoring function \\(f(q, d_i)\\) similar to a pointwise formulation. However, the key difference arises in how the training data is consructed - where we take pairs of documents within the same query as training samples.\n\n\\(x_1 \\to q_1, (d_1, d_2)\\)\n\\(x_2 \\to q_1, (d_3, d_4)\\)\n\\(x_3 \\to q_1, (d_3, d_5)\\)\n\\(x_4 \\to q_1, (d_4, d_5)\\)\n\nIn this setting, a new set of pairwise binary labels are derived, by comapring the individual relevance score in each pair. For example, given the first query \\(q_1\\), if \\(y_1==\\) (i.e.. an irrelevant document) for \\(d_1\\) & \\(y_2=3\\) (i.e.. a Highly relevant document) for \\(d_2\\) then we can create a new label \\(y_1<y_2\\) for the pair of docs \\((d_1, d_2)\\) - by doing so we have essentially converted this back to a binary classification task again.\nNow in order to learn the function \\(f(q, d_i)\\) which is still pointwise, but in a pairwise manner, we model the difference in scores probablistically as follows:\n\\[P(i>j) \\equiv \\frac{1}{1+exp^{-(s_i - s_j)}}\\]\ni.e, if document \\(i\\) is better matched than document \\(j\\) (denoted as \\(i>j\\)), then the probability of the scoring function to have scored \\(f(q, d_i) = S_i\\) should be close to 1. In other words, the model is trying to learn, given a query, how to score a pair of documents such that a more relevant document would be scored higher."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#listwise-method",
    "href": "posts/post-1/marketplace-recsys-part-1.html#listwise-method",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Listwise method",
    "text": "Listwise method\nListwise methods, solve the problem of ranking by learning to score the entire list jointly and they do so via two main sub techniques:\n\nDirect optimization of IR measures such as NDCG.(Eg: SoftRank, AdaRank).\nMinimize a loss function that is defined based on understanding the unique properties of the kind of ranking you are trying to achieve. (E.g. ListNet, ListMLE).\n\nLet‚Äôs do a quick review of one of these approaches:\nConsider ListNet, Which is based on the concept of permutation probability of a list of items. In this case we assume there is a pointwise scoring function \\(f(q,di)\\) used to score and rank a given list of items. But instead of modeling the probability as a pairwise comparison using scoring difference, we model the probability of the entire list of results. In this setting our documents and query dataset would appear like this:\n\n\\(x_1 : q_1, (d_1, d_2)\\)\n\\(x_2 : q_2, (d_3, d_4, d_5)\\)\n\nFirst let‚Äôs look at the Permutation probability. Let‚Äôs denote \\(œÄ\\) as a specific permutation of a given list of length \\(n\\), \\(\\Theta (s_i) = f(q, d_i)\\) as any increasing function of scoring \\(s_i\\) given a query \\(q\\) and a document \\(i\\). The probability of having a permutation \\(œÄ\\) can be written as follows:\n\\[P(\\pi) = \\prod_{i=1}^n \\frac{\\phi(s_i)}{\\sum_{k=i}^n\\phi(s_k)}\\]\nTo illustrate consider a list of 3 items, then the probability of returning the permutation \\(s_1\\),\\(s_2\\),\\(s_3\\) is calculated as follows:\n\\[P(\\pi = \\{s_1, s_2, s_3\\}) = \\frac{\\phi(s_1)}{\\phi(s_1) + \\phi(s_2) + \\phi(s_3)} \\cdot \\frac{\\phi(s_2)}{\\phi(s_2) + \\phi(s_3)} \\cdot \\frac{\\phi(s_3)}{\\phi(s_3)}\\]\nDue to computational complexity, ListNet simplies the problem by looking at only the top-one probability of a given item. The top-one probability of object i equals the sum of the permutation probabilities of permutations in which object i is ranked on the top. Indeed, the top-one probability of object i can be written as:\n\\[P(i) = \\frac{\\phi(s_i)}{\\sum_{k=1}^n \\phi(s_k)}\\]\nGiven any two list of items represented by top-one probabilities, we can now measure the difference between them using cross entropy. Then we can use an ml algorithm which minimises that cross entropy. The choice of function \\(œï(‚ãÖ)\\), can be as simple as just an exponential function. When \\(œï(‚ãÖ)\\) is expotential and the list length is two, the solution basically reduces to a pairwise method as described earlier.\nWith that brief introduction out of the way, let‚Äôs also quickly look at the advantages and disadvantages of each of these approaches:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nPointwise\n\nSimplicity, Any standard ML models can be applied without any changes.\n\n\nThe results can be often sub-optimal due to not utilizing the full information of the entire list of matching documents for each query.\nThis requires explicit labels while constructing the dataset, and can be quite expensive.\n\n\n\nPairwise\n\nWe don‚Äôt need explicit labels, Only pairwise preferences are required.\nThe model learns how to rank directly, even though its a pairwise setting, but in theory it can approximate the performance of a general ranking task.\n\n\nThe Core Scoring mechanism is still pointwise. Hence, that relative information in the feature space among different documents given the same query is still not fully leveraged.\n\n\n\nListwise\n\nTheoretically this is a good solution for a ranking task.\n\n\nCostly to compute in its theoretical form and hence several approximations are used in practice.\nScoring function is still pointwise, which could be sub-optimal."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#mean-average-precision-map",
    "href": "posts/post-1/marketplace-recsys-part-1.html#mean-average-precision-map",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Mean Average Precision (MAP)",
    "text": "Mean Average Precision (MAP)\nMAP is a measure based on binary label of relevancy. To compute this first we define precision at k for a given query \\(P@k(q)\\) as:\n\\[P@k(q) \\equiv \\frac{\\sum_{i=1}^k r_i}{k}\\]\nfor an ordered list of prediction \\(r_i\\) for all \\(k\\) items. \\(ri=1\\) if it is relevant and 0 otherwise.Then we define the average precision given a query \\(AP(q)\\) at \\(k\\) items as:\n\\[AP(q)@k \\equiv \\frac{1}{\\sum_{i=1}^k r_i} \\sum_{i=1}^k P@i(q) \\times r_i\\]\nMean Average Precision is just the mean of \\(AP(q)\\) for all queries:\n\\[MAP \\equiv \\frac{\\sum_{q=1}^Q AP(q)}{Q}\\]\nAlso, MAP is an order sensitive metric because of the term \\(r_i\\) in the calculation of AP. It is essentially taking the average of precision at each ranking position and penalizing the precision at positions with irrelevant item by strcitly setting them to zeroes.\nHere is a simple example for computing MAP:"
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#mean-reciprocal-rank-mrr-expected-reciprocal-rank-err",
    "href": "posts/post-1/marketplace-recsys-part-1.html#mean-reciprocal-rank-mrr-expected-reciprocal-rank-err",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Mean Reciprocal Rank (MRR) & Expected Reciprocal Rank (ERR)",
    "text": "Mean Reciprocal Rank (MRR) & Expected Reciprocal Rank (ERR)\nReciprocal rank metrics focus mainly on the first correctly predicted relevant item in a list. Given a list of items, and say \\(r_i\\) is the rank of the highest ranking relevant item & if the the 2nd item is the first relevant item in the list, then the reciprocal rank for this query would be \\(\\frac{1}{2}\\). By extension, each query will have a reciprocal rank. Hence, Mean reciprocal rank is essentially the average of reciprocal rank for all the queries, which would be represented as follows:\n\\[MRR \\equiv \\frac{1}{Q} \\sum_{i=1}^Q\\frac{1}{r_i}\\]\nExpected reciprocal rank tries to quantify how useful a document at rank \\(i\\) conditioned on the degree of relevance of documents at rank less than \\(i\\) are. The intution behind this is based on the empirical findings from web search task, that the likelihood a user will examine the document at rank \\(i\\) is dependent on how satisfied the user was with previously observed documents in the list.\nLets assume the probability of a user finding the result is satisfied at position \\(i\\) in a list of items is denoted as \\(R_i\\) & the likelihood of a session for which the user is satisfied and stops at position \\(r\\) is: \\[\\prod_{i=1}^{r-1}(1 - R_i)R_r\\]\nNow we can model \\(R_i\\) such that it is an increasing function of relevance:\n\\[R = R(g) \\equiv \\frac{2^g - 1}{2^{g_{max}}}\\]\nwhere \\(g\\) is the graded relevance such that \\(g \\in \\{0, 1, ..., g_{max}\\}\\) & \\(g = 0\\) implies an irrelevant document and \\(g = g_{max}\\) implies a relevant document.\nNow we can define ERR as follows:\n\\[ERR \\equiv \\sum_{r=1}^n\\frac{1}{r}R_r\\prod_{i=1}^{r-1}(1-R_i)\\]\nHere \\(\\frac{1}{r}\\) is treated as a utility function \\(\\tau(r)\\) that satisfies \\(\\tau(1) = 1\\) and \\(\\tau(r) \\rightarrow 0\\) as \\(r \\rightarrow \\infty\\).\nNote that ERR is a metric on a list with a single query, To evaluate results from multiple queries, we will need to further average ERRs among queries.\nHere is a simple example for computing MRR:"
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#normalized-discounted-cumulative-gain-ndcg",
    "href": "posts/post-1/marketplace-recsys-part-1.html#normalized-discounted-cumulative-gain-ndcg",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Normalized Discounted Cumulative Gain (NDCG)",
    "text": "Normalized Discounted Cumulative Gain (NDCG)\nNormalized Discounted Cumulative Gain (NDCG) is one of the most popular metric for measuring the quality of a set of ranked items in search or recommendations. If we were to break the assumptions made by this metric in simple terms, it would be as follows:\n\nCumulative Gain: Very relevant items are more useful than somewhat relevant items which are more useful than completely irrelevant items.\nDiscounting: Relevant items are more useful when they appear earlier in a list of ranked items.\nNormalization: The result of the ranking should be irrelevant to the query performed.\n\nLet‚Äôs define Discounted Cumulative Gain at position \\(k\\) as follows:\n\\[DCG@k \\equiv \\sum_{i=1}^k\\frac{2^{l_i} - 1}{log_2(i + 1)}\\]\nwhere \\(l_i\\) is the grading of relevance at rank \\(i\\). Intutively, the numerator is simply an increasing function of relevance, the more relevant the higher. This is the gain from each item. The denominator is a decreasing function of ranking position, this is the discounted component of the metric. Collectively, higher relevance gains more score, but the lower it is ranked the higher also the discount. Essentially, the metric will prefer higher relevant item to be ranked higher, which is the desired outcome.\nNDCG is then defined as:\n\\[NDCG@k = \\frac{DCG@k}{IDCG@k}\\]\nwhere \\(IDCG@k\\) is the Ideal DCG@k given the result. DCG@k is calculated by sorting the given list of items by its true relevance labels. and IDCG@k is the maximum possible DCG@K value one can get given a ranked list of items.\nHere is a simple example for computing NDCG:"
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#moo---the-what-the-why-the-how.",
    "href": "posts/post-1/marketplace-recsys-part-1.html#moo---the-what-the-why-the-how.",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "MOO - The What, The Why & The How.",
    "text": "MOO - The What, The Why & The How.\nAfter that long detour, let‚Äôs now look into Multiple Objective Optimisation applied to Ranking problem within a marketplace type setting involving multiple stakeholders. Let‚Äôs do so with a simple illustrative example that involves similar item recommendations. Specifically, let‚Äôs understand Why this is needed first.\nPicture this, A User has browsed and explored a bunch of different red shirts that he‚Äôs planning to buy, and on one such open tab we‚Äôve got a shirt that he‚Äôs closely examining. Now, the recommendation system in the backend is also generating a bunch of similar shirts that he might be interested in, further let‚Äôs say the recommendation system is optimising for two different objectives - firstly, to show not just relevant but also personalised shirts similar to his style, based on his interaction history that he might explore further & perhaps buy. Secondly, the platform also wants to make sure it‚Äôs making a good profit on each sales that the recommendation system generates/leads to - hence wants to also promote shirts with a good margin. How would we do this?\nLet‚Äôs start by defining our objectives:\n\n\n\n\n\n\n\nObjective I\nObjective II\n\n\n\n\nPersonalisation\nPrice Margin\n\n\n\nLet‚Äôs also say the item options available to us to be displayed (let‚Äôs focus on finding the top-1 item at the moment.) if plotted on a graph look something like this:\n\nNow, essentially we want to maximise profit margin but also we want to maximise for highly personalised & relevant shirt. If we were to solve this from a lens of single objectve optimisation two scenarios would arise:\n\nCase-I: Applying 1D Optimisation to objective I followed by optimising for objective II, i.e First we can find Highly Personalised shirts and then search for the shirt that also provides maximum margin.\nCase-II: Applying 1D Optimisation to objective II followed by optimising for objective I, i.e First we can find High Profit margin shirts and then search for the ones which are also Highly Personalised & relevant for the user‚Äôs style.\n\nFor a simple and better understanding of the concept, assume that someone is trying to find the best red shirt from both the perspectives manually in a physical store, and let‚Äôs further also assume its a dark room.\n\nCase I : Highly Personalised & High Profit margin\n\n\nSo, in this case the solver turns the torch on towards the relevance axis and discovers \\(T5\\) as the most suitable candidate.\nNow from this point the solver turns towards the profit axis and turns the torch on and discovers \\(T6\\).\n\n\n\nCase II : High Profit margin & High Personalisation\n\n\nIn this case the solver turns the torch on towards the profit axis and discovers \\(T3\\) as the best candidate.\nNow from this point the solver turns towards the relevance axis and turns the torch on and discovers \\(T4\\).\n\nWhat do we have as of now:\n\n\n\n\n\n\n\n\n\nProcess\n1st Optimization Objective\n2nd Optimization Objective\nOptimal Solution\n\n\n\n\n1\nPersonalisation\nProfit Margin\n\\(T6\\)\n\n\n2\nProfit Margin\nPersonalisation\n\\(T4\\)\n\n\n\nSo, that seems odd. there‚Äôs multiple solutions depending on what we actually want to achieve. If, we want to optimise for both and find the best solution, Going about in the above method seems to be not the best way. This is why, we need to optimise for finding solutions for both objectives collectively and the method or set of optimisation methods for it is called Multiple Objective Optimization.\nLet‚Äôs define what MOO is & try to understand some other associated important concepts first, before we see how these are applied to ranking task in industry.\n\n\n\n\n\n\nBasic Concepts - Defining the MOO Problem, Pareto Optimum & Pareto Frontier\n\n\n\n\n\nMOO: Optimization that involves identifying the values of decision (or free) variables that generate the maximum or minimum of one or more objectives. In most engineering problems, there may exist multiple, conflicting objectives and solving the optimization problems is not trivial or may not be even feasible sometimes. One way to formulate this is as follows:\nGiven \\(m\\) inequality constraints & \\(p\\) equality constraints identify a vector \\(\\bar{x}^*_n = [x^*_1, x^*_2,...,x^*_n]^T\\) that optimizes\n\\[\\bar{f}_k(\\bar{x}_n) = [o_1(\\bar{x}_n), o_2(\\bar{x}_n),..., o_k(\\bar{x}_n)]^T\\]\nsuch that\n\\[g_i(\\bar{x}_n) ‚â• 0, i = 1, 2, . . . , m\\]\n\\[h_i( \\bar{x}_n) = 0, i = 1, 2, . . . , p\\]\nwhere \\(\\bar{x}_n = [x_1, x_2, . . . , x_n]^T\\) is a vector of \\(n\\) decision variables. The constraints determine the ‚Äúfeasible region‚Äù \\(F\\) and any point \\(\\bar{x}_n ‚àà F\\) gives a ‚Äúfeasible solution‚Äù where \\(g_i( \\bar{x}_n)\\) and \\(h_i( \\bar{x}_n)\\) are the constraints imposed on decision variables. The vector function \\(\\bar{f}_k(\\bar{x}_n)\\) above is a set of \\(k\\) objective functions, \\(o_i(\\bar{x}_n)\\) for \\(i = 1, ¬∑ ¬∑ ¬∑ , k\\), representing k non-commensurable criteria.\nPareto Optimum: A point \\(\\bar{x}^‚àó\\) is ‚ÄúPareto optimal‚Äù (for minimisation task) if the following holds for every \\(\\bar{x}_n ‚àà F\\)\n\\[\\bar{f}_k(\\bar{x}^*_n) ‚â§  \\bar{f}_k( \\bar{x}_n)\\]\nwhere \\(\\bar{f}_k(\\bar{x}_n) = [o_1(\\bar{x}_n), o_2(\\bar{x}_n),..., o_k(\\bar{x}_n)]^T\\), \\(\\bar{f}^*_k(\\bar{x}^*_n) = [o_1(\\bar{x}^*_n), o_2(\\bar{x}^*_n),..., o_k(\\bar{x}^*_n)]^T\\)\nPareto optimality gives a set of nondominated solutions. A feasible solution \\(x\\) is called ‚Äúweakly nondominated‚Äù if there is no \\(y ‚àà F\\) , such that \\(o_i(y) < o_i(x)\\) for all \\(i = 1, 2, ..., k\\). This means that there is no other feasible solution that can strictly dominate \\(x\\). A feasible solution \\(x\\) is called ‚Äústrongly nondominated‚Äù if there is no \\(y ‚àà F\\) , such that \\(o_i(y) ‚â§ o_i(x)\\) for all \\(i = 1, 2, ...k\\), and \\(o_i(y) < o_i(x)\\) for at least one \\(i\\). This means that there is no other feasible solution that can improve some objectives without worsening at least one other objective. If \\(x\\) is ‚Äústrongly nondominated‚Äù, it is also ‚Äúweakly nondominated‚Äù.\nPareto Frontier: A set (of feasible solutions) that is Pareto efficient is called the Pareto frontier, Pareto set, or Pareto front. The optimal solutions can be determined based on the tradeoffs within this set based on a designer‚Äôs decisions for acceptable performance.\nread more on this from A Survey on Modeling and Optimizing Multi-Objective Systems"
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#popular-methods-applied-to-ltr-setting.",
    "href": "posts/post-1/marketplace-recsys-part-1.html#popular-methods-applied-to-ltr-setting.",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Popular methods applied to LTR Setting.",
    "text": "Popular methods applied to LTR Setting.\nOver the years, there have been numerous approaches that have been used in LTR setting. Broadly most of these approaches, can be classified into four categories as follows:\n\nLabel Aggregation\nConstraint Optimization\nModel Fusion/Aggregation\nLexicographic\n\nNow, lets us define these approaches."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#label-aggregation",
    "href": "posts/post-1/marketplace-recsys-part-1.html#label-aggregation",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Label Aggregation",
    "text": "Label Aggregation\nIn this method, we combine all objective functions to form a single objective function which is then minimized. Once we convert the multi-objective into a single objective by combing the labels, we solved the given LTR problem as the single objective function. Let‚Äôs say for given query q, and products P, we have two different labels, \\(‚Ñì_1(q,p)\\) a similarity score between query and prodct, and \\(‚Ñì_2(p)\\) a profit margin for a product, we can put a new label as \\(‚Ñì(q, p) = \\alpha¬∑‚Ñì_1(q, p) + (1 ‚àí \\alpha)¬∑‚Ñì2(p)\\)\nNote: Here \\(\\alpha\\) is manually chosen by the user. If \\(\\alpha \\in \\{0,1\\}\\), the problem is reduced to a single objective optimization.\nIn general, given k objectives rank function looks like below:\nMinimise,\n\\[f(x) = \\sum_{i=1}^k{\\alpha_i * f_i(x)}\\]\nSubject to \\[\\sum_{i=1}^k{\\alpha_i = 1}\\]\n\nAdvantages\n\nIt gives a clear interpretation of the multi-objective function and generalize it.\nIt allows multiple parameters to be set to reflect preferences.\n\n\n\nDisadvantages\n\nIt tries to optimize for each objective function, which can be computationally expensive.\nThe setting of parameters \\((\\alpha_i)\\) is not intuitively clear when only one solution point is desired."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#constraint-optimization",
    "href": "posts/post-1/marketplace-recsys-part-1.html#constraint-optimization",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Constraint Optimization",
    "text": "Constraint Optimization\nThis method optimizes the single most important objective \\(f_{primary}(x)\\) and treat the other objectives as constraints with pre-determined upperbound. As we saw eariler, let‚Äôs say have two objectives to optimize. First objective is \\(‚Ñì_1(q,p)\\) - similarity score between \\((p,q)\\) and second \\(‚Ñì_2(p)\\) - profit margin for \\(o\\). Then we could consider \\(‚Ñì_1(q,p)\\) as primary objective and \\(‚Ñì_2(p)\\) as secondary objective. In this method we will optimize for primary objective \\(‚Ñì_1(q,p)\\) subject to \\(‚Ñì_2(p) \\lesssim \\epsilon\\).\nIn general, given k objectives the ranking function is as follows:\n\\[ \\min_{\\forall x_i} f_l(x)\\]\nSubject to \\[ f_i(x) \\lesssim \\epsilon_i, \\forall i \\not =  l ,\\] \\(\\epsilon_i\\) is upperbound for \\(f_i(x)\\). And, \\(f_l(x)\\) is the primary function to optimize.\n\nAdvantages\n\nIt focuses on a single objective with limits on others.\nIt always provides a weakly Pareto optimal point, assuming that the formulation gives a solution.\nIt is not necessary to normalize the objective functions.\n\n\n\nDisadvantages\n\nThe optimization problem may be infeasible if the bounds on the objective functions are not appropriate."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#model-fusionaggregation",
    "href": "posts/post-1/marketplace-recsys-part-1.html#model-fusionaggregation",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Model Fusion/Aggregation",
    "text": "Model Fusion/Aggregation\nThis method is an aggregation of multiple independent ranking models. The final ranking socre is obtained by a convex combination of multiple models. As we saw earlier if we have two objectives, let‚Äôs say \\(‚Ñì_1(q,p)\\) - similarity between query and product and \\(‚Ñì_2(p)\\) - profit margin. Then first we train the \\(M_{l1}\\) model which optimizes for similarity score between (q,p). And further we also independently train another model \\(M_{l2}\\) which optimizes for the profit margin. The linear combination of the models can be formulated as \\(M(q,p) = \\alpha¬∑M_{l1}(q, p) + (1 ‚àí Œ±)¬∑M_{l2}(p)\\), where the hyperparameter \\(\\alpha \\in[0, .,., 1]\\) controls the tradeoff between the two model scores.\nIn general, given k objectives the ranking function looks as follows:\n\\[M(x) = \\sum_{i=1}^k (\\alpha_i * M_i (x))\\]\nwhere, \\(M_i(x)\\) is an independently trained model for optimizing \\(i^{th}\\) objective.\n\nAdvantages\n\nThis is used as a post-rank method, and as such easy to tweak weighting parameters.\nLearning for one single objective will not be affected by other objectives (decoupled objectives).\n\n\n\nDisadvantages\n\nIt‚Äôs difficult to find the optimal weight for the final ranking."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#lexicographic",
    "href": "posts/post-1/marketplace-recsys-part-1.html#lexicographic",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Lexicographic",
    "text": "Lexicographic\nWhen we have more than one objective, we rank the items by ordering the objective functions according to their importance. As mentioned earlier, we have two objective functions, \\(‚Ñì_1(q,p)\\) similarity between query and product as pprimary objective and \\(‚Ñì_2(p)\\) profit margin as the secondary objective. Then We will order the items according to the primary objective \\(‚Ñì_1(q,p)\\) and if a tie happens, then we use \\(‚Ñì_2(p)\\) the secondary objective score to break the tie.\nIn general, given k objectives, the rank function looks like below:\nMinimise,\n\\[ f_i(x)\\]\nSubject to \\[ f_j(x) \\lesssim f_j(x_j^*)\\] \\(j = 1\\) to (\\(i - 1\\)) and \\(i > 1\\) ; \\(i = 1\\) to \\(k\\)\nHere, we rank the function based on \\(f_i\\), and where a tie occurs we break the tie based on \\(f_{(i+1)}\\) score.\n\nAdvantages\n\nIt is a unique approach to specifying preferences.\nIt does not require that the objective functions be normalized.\n\n\n\nDisadvantages\n\nIt requires that additional constraints be imposed.\nIt is computation heavy if we have more objectives."
  },
  {
    "objectID": "posts/post-1/marketplace-recsys-part-1.html#few-examples-in-the-wild",
    "href": "posts/post-1/marketplace-recsys-part-1.html#few-examples-in-the-wild",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Few Examples in the Wild",
    "text": "Few Examples in the Wild\n\nA Multi-Objective Learning to Re-Rank Approach to Optimize Online Marketplaces for Multiple Stakeholders.\n\n\n\n\n\n\nRead the Original Paper over here: Original Paper\n\n\n\nFirst, let‚Äôs look at an example from Expedia. Being a service provider they are trying to optimize for the following objectives:\n\nConsumer‚Äôs Conversion Rate or Click Through Rate.\nMaximum transaction commission from the supplier.\n\nA supplier pays a marginal amount of the sales transaction as a commission to the platform. Let‚Äôs say the product‚Äôs cost is \\(c\\) and the selling price is \\(p\\) then the margin is \\(m = p-c\\).\nTo achieve the above objectives, the optimization function is defined as follows:\n\\[\\max_{\\alpha, \\beta}L(m|u) = \\sum_{i=1}^n \\log(u_i) + \\alpha \\log(p_i) + \\beta \\log(\\frac{m_i}{p_i})\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are tuning parameters.\n\\(\\log(u_i)\\) = User possibility to do transaction.\n\\(\\log(p_i)\\) = supplier interest for selling the product at price \\(p\\).\n\\(\\log(m/p)\\) = profit margin define as percentage here.\nIn these objective functions, we have contradictory relations with suppliers and consumers by nature. On the other side, service providers want higher commission benefits from suppliers at an indirect cost to consumers. For such problems, there is no unique Pareto optimal solution because of the complex and opposing interactions of the different stakeholder interests.\nHere they give priority to promoting items to higher positions that better satisfy a transaction commission objective while staying as close as possible to the original ranking. The new ranking score function looks like the below:\n\\[ \\forall_u \\in u, u^{‚Ä≤} = u + \\alpha\\log(p) + \\beta(x,m)\\log(\\frac{m}{p})\\]\nTo reduce the distance between the new vs old score vectors, they use the Kendall tau correlation measure. The updated objective function looks like the below:\n\\[\\min_{\\alpha, \\beta}L(m|u, X) = L_r(\\frac{m}{X}) + \\gamma(1 - K^{'}(u, u^{‚Ä≤})) \\]\nHere, \\(K^{'}(u, u^{'})\\) is playing the role of a similarity-based regularizer with the original ranking order u being the reference point to the new ranking order \\(u^{‚Ä≤}\\). The hyperparameter \\(\\gamma\\) gives the balance between our objective function.\nTo evaluate the LTR method, they use an in-house Expedia dataset built from worldwide hotel searches collected during 2016. The new ranking methods give a lift¬â of +16.7% on the NDCG of margin, but at the same time, this incurs a decrease of -5.9% in terms of the NDCG of customer preferences.\nThe paper also uses the following formula to calculate the risk and reward to further evaluate the new objective function:\n\\[ Risk = \\frac{1}{|q|} \\sum_q NDCG@10(LRR) < NDCG@10(Baseline)\\]\n\\[ Reward = \\frac{1}{|q|} \\sum_q NDCG@10(LRR) > NDCG@10(Baseline)\\]\nThey have found that the risk is lower than the baseline and the reward much higher.\nThis experiment shows that multi-objective is always a trade-off between the two objectives: Conversion rate vs Margin. They would like to further test with real-world performance via \\(A/B\\) testing.\n\n\nJoint Optimization of Profit and Relevance for Recommendation Systems in E-commerce\n\n\n\n\n\n\nRead the Original Paper over here: Original Paper\n\n\n\nTraditionally, e-commerce-based recommendation systems focus on optimizing for relevance by predicting the purchase or click probability of an item. The relevance-centric approach is capable to increased the conversion rate but does not guarantee a maximum profit for either a platform or seller. In this paper from Etsy, where they discussed a novel revenue model, which optimises for two objectives:\n\nMaximising the Revenue\nThe probability of the purchase.\n\nTo maximise the likelihood, the objective function is defined as follows:\n\\[\\max_{w \\in R^n, b \\in R} l(w,b) := - \\sum_{i=1}^{m} \\log(1 + exp(-y_i(w^T x_i + b)))\\]\nHere, \\(x_i\\) = \\(i^{th}\\) training sample \\(x_i \\in R^n\\) \\(y_i\\) = label whether a recommended item is purchased or not\nLet‚Äôs say \\(\\pi_i\\) is the price of item, then the objective function for maximizing profit is as below:\n\\[œÅ(w,b) := \\sum_{i=1}^{m} E[\\pi_i, y_i] = \\sum_{i=1}^{m} \\pi_i (2prob[y_i = 1|x_i;w] - 1)\\]\n\\[= \\sum_{i=1}^{m} 2\\pi_i\\sigma(w^T x_i + b) - \\pi_i,\\]\nThe combined objective function will be:\n\\[\\max_{w\\in R^n, b\\in R } l(w,b) + \\mu œÅ(w,b)\\]\nTo optimise the above objective, \\(w\\), $\\(b\\) parameters need to be found which can fit \\(l(w,b)\\) to maximise the likelihood while maximizing the expected revenue (via \\(œÅ(w, b)\\)). Here, \\(\\mu ‚â• 0\\) is a hyperparameter of the model that controls the tradeoff between the two objectives. Once the optimal parameters are learned, they use them to rank a set of candidate items.\nThey have used the following metrics to evaluate the performance of the proposed model\n\nProfit@K: profit generate by kth highest ranked item\nAverage price: Average price of the k highest ranked item\nAUC: to measure relevance\nNDCG@5\n\nEtsy has done an offline evaluation of the proposed model. They found the proposed revenue model attains the highest AUC and profit@k for all K values. Note they used K value up to 3. They observe that the revenue model has a 3.57% increase in AUC and 9.50% in profit@1 compared to the baseline model. Also increases P-NDCG@k and AP@k for all k by at least 3.57% and 23.08% However, the proposed model results in a 10.76% and 16.06% decrease in AUC compared to the baseline.\nThe overall revenue model can increase profit for the platform while retaining high relevancy for users. They would like to assess revenue model performance in the face of real user traffic via an online A/B experiment.\n\n\nUsing Bayesian optimization for balancing metrics in recommendation systems\n\n\n\n\n\n\nRead the Original article over here: Original Article\n\n\n\nLet‚Äôs now look into an article from Linkedin where they describe their approach towards building their notifications recommendation system, which notifies members about various activities within their network. To build an efficient notification system, they are optimizing for the following objects:\n\nCTR\nNumber of sessions (i.e the user visits & app open rate)\n\nThe CTR and session objectives can be conflicting, because sending more notifications to members may increase the overall number of sessions, but can decrease CTR because of the quality of the notification.\nThe overall objective function they define is as follows:\n\\[P_{click} + \\alpha * \\delta P_{visit} > \\gamma\\]\nwhere,\n\n\\(p_{click}\\) is the probability of click by member\n\\(\\delta P_{visit}\\) can be defined as \\(p(\\frac{visit}{sent})\\) - \\(p(\\frac{visit}{not sent})\\). Which is basically the difference between sending a notification now vs not sending a notification.\n\\(a\\) is the hyperparameter that measures the relative importance of the two utilities.\n\\(ùõæ\\) is the threshold applied.\n\nFinding an optimal value of \\(x={a,ùõæ}\\) can maximize the sessions without compromising on CTR and send volume. To ensure the overall performance, they used \\(c1\\) and \\(c2\\) constraints as contained for CTR(x), and Send Volume(x).\nThe updated objective function looks like the below:\n\\[ \\max_{\\forall x_i} , f_{session}(x)\\]\nSuch that, \\[f_{CTR}(x) > c_1, f_{sent-volume} (x) < c_2\\]\nUsing the above constraints, they define a single objective function as below:\n\\[\\max_x U(x) = f_{session}(x) + \\lambda(1 \\{f_{CTR}(x) > c_1 \\} + 1\\{f_{sent-volume(x)} < c_2\\}),\\]\nBy converting a MOO into SOO using constraint-based optimization, they learn global hyperparameters (i.e., a single value) for all members. Based on online A/B experiments, they found that learning separate hyperparameters for cohorts of members gives better results than converting a MOO into SOO."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "The Foundation: A Notes on Recsys, LTR, Ranking Evaluation metrics & Multi Objective Ranking in practice.\n\n\n\n\nltr\n\n\nrecsys\n\n\noptimisation\n\n\napplied-ml\n\n\nmarketplace\n\n\neconomics\n\n\nnotes\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2022\n\n\nBharath G.S & Rita Anjana\n\n\n\n\n\n\nNo matching items"
  }
]