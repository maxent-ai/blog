[
  {
    "objectID": "posts/post-1/rank-with-moo-1.html#stakeholders-objectives-the-objective-interplay",
    "href": "posts/post-1/rank-with-moo-1.html#stakeholders-objectives-the-objective-interplay",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Stakeholders, Objectives & the Objective interplay",
    "text": "Stakeholders, Objectives & the Objective interplay\nIn a multi-sided system with multiple objectives often the objectives interact and behave in different manners. At a high level, you could categorize the different types of these interplay as follows:\n\nCorrelated : Optimising for one objective helps the other.\nNeutral: Optimising for one does not impact the other\nAnti Correlated: Optimising for one hurts the other\n\nConsider the example of our Hypothetical on-demand delivery startup Hyper, which is a three-sided marketplace - i.e it has three sets of stakeholders with different motivations & objectives.\n\n\n\n\n\n\n\n\nStakeholder\nNeeds/Motivations\nPotential Objectives\n\n\n\n\nEnd User\nWants to order something from local partner shops.\n\nQuick delivery\nBest price\nReliable merchants\nFresh items\n\n\n\nMerchants\nProvide online visibility and find customers.\n\nMatching quality\nExposure\nMinimise wastage\n\n\n\nDelivery Partners\nEarn a stable livelihood.\n\nRegularity in jobs\nEarnings per partner\nEfficient drop location planning\n\n\n\n\nAs we see above each entity has a unique set of potential objectives one could optimise the recommendation system for & this has to be done in a deliberate and careful way or it might result in undesired outcomes.\n\n\n\n\n\n\nüßµ Superstar economics and towards a fair marketplace\n\n\n\n\n\nResearchers from Microsoft & Spotify found out that:\n\nRecommendation systems in general suffer from what is called as an inherent problem of ‚Äúsuperstar economics‚Äù: rankings have a top and a tail end, not just for popularity, but also for relevance. In an attempt to maximize user satisfaction, recommender system optimize for relevance. This inadvertently leads to lock-in of popular and relevant suppliers, especially for users who want to minimize the effort required to interact with the system. A major side-effect of the superstar economics is the impedance to suppliers on the tail-end of the spectrum, who struggle to attract consumers, given the low exposure, and thus, are not satisfied with the marketplace. Indeed, to continue to attract more suppliers to the platform, marketplaces face an interesting problem of optimizing their models for supplier exposure, and visibility. Indeed, the suppliers (e.g.¬†retailers, artists) would want a fair opportunity to be presented to the users.\n\nread more about this in Towards a Fair Marketplace: Counterfactual Evaluation of the trade-off between Relevance, Fairness & Satisfaction in Recommendation Systems\n\n\n\nIn short, what a system optimises for is super important & in general, it needs to look for a balance while optimising for various objectives.\nBefore we proceed further on the track of optimising for multiple objectives (or multiple objective optimisation) - let us quickly visit how most modern recommendation systems look like & also do a quick recap of how one builds a model for ranking (aka Learning to rank) & some key metrics used for measuring these systems."
  },
  {
    "objectID": "posts/post-1/rank-with-moo-1.html#ranking-task",
    "href": "posts/post-1/rank-with-moo-1.html#ranking-task",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Ranking Task",
    "text": "Ranking Task\nGiven a query \\(q\\), and a set of \\(n\\) documents \\(D=d_1,d_2,...,dn\\), we‚Äôd like to learn a function \\(f\\) such that \\(f(q, D)\\) will predict the relevance of any given document associated with a query."
  },
  {
    "objectID": "posts/post-1/rank-with-moo-1.html#pointwise-method",
    "href": "posts/post-1/rank-with-moo-1.html#pointwise-method",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Pointwise method",
    "text": "Pointwise method\nIn pointwise method, the above ranking task is re-formulated as a standard classification/regression task. The function to be learned \\(f(q, D)\\) is simplified as \\(f(q, d_i)\\) i.e the relevance of each document given a query is scored independently.\nFor instance, if we have two queries associated with 2 and 3 resulting matched documents:\n\n\\(q_1 \\to d_1, d_2\\)\n\\(q_2 \\to d_3, d_4, d_5\\)\n\nThen the training data \\(x_i\\) in a pointwise method will essentially be every query-document pair as follows:\n\n\\(x_1 : q_1, d_1\\)\n\\(x_2 : q_1, d_2\\)\n\\(x_3 : q_2, d_3\\)\n\\(x_4 : q_2, d_4\\)\n\\(x_5 : q_2, d_5\\)\n\nSince each document is scored independently with the absolute relevance as the target label, the task is no different from any standard classification or regression task. As such any standard ml algorithms can be leveraged in this setting."
  },
  {
    "objectID": "posts/post-1/rank-with-moo-1.html#pairwise-method",
    "href": "posts/post-1/rank-with-moo-1.html#pairwise-method",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Pairwise method",
    "text": "Pairwise method\nIn pairwise method, the goal is to learn a pointwise scoring function \\(f(q, d_i)\\) similar to a pointwise formulation. However, the key difference arises in how the training data is consructed - where we take pairs of documents within the same query as training samples.\n\n\\(x_1 \\to q_1, (d_1, d_2)\\)\n\\(x_2 \\to q_1, (d_3, d_4)\\)\n\\(x_3 \\to q_1, (d_3, d_5)\\)\n\\(x_4 \\to q_1, (d_4, d_5)\\)\n\nIn this setting, a new set of pairwise binary labels are derived, by comapring the individual relevance score in each pair. For example, given the first query \\(q_1\\), if \\(y_1==\\) (i.e.. an irrelevant document) for \\(d_1\\) & \\(y_2=3\\) (i.e.. a Highly relevant document) for \\(d_2\\) then we can create a new label \\(y_1<y_2\\) for the pair of docs \\((d_1, d_2)\\) - by doing so we have essentially converted this back to a binary classification task again.\nNow in order to learn the function \\(f(q, d_i)\\) which is still pointwise, but in a pairwise manner, we model the difference in scores probablistically as follows:\n\\[P(i>j) \\equiv \\frac{1}{1+exp^{-(s_i - s_j)}}\\]\ni.e, if document \\(i\\) is better matched than document \\(j\\) (denoted as \\(i>j\\)), then the probability of the scoring function to have scored \\(f(q, d_i) = S_i\\) should be close to 1. In other words, the model is trying to learn, given a query, how to score a pair of documents such that a more relevant document would be scored higher."
  },
  {
    "objectID": "posts/post-1/rank-with-moo-1.html#listwise-method",
    "href": "posts/post-1/rank-with-moo-1.html#listwise-method",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Listwise method",
    "text": "Listwise method\nListwise methods, solve the problem of ranking by learning to score the entire list jointly and they do so via two main sub techniques:\n\nDirect optimization of IR measures such as NDCG.(Eg: SoftRank, AdaRank).\nMinimize a loss function that is defined based on understanding the unique properties of the kind of ranking you are trying to achieve. (E.g. ListNet, ListMLE).\n\nLet‚Äôs do a quick review of one of these approaches:\nConsider ListNet, Which is based on the concept of permutation probability of a list of items. In this case we assume there is a pointwise scoring function \\(f(q,di)\\) used to score and rank a given list of items. But instead of modeling the probability as a pairwise comparison using scoring difference, we model the probability of the entire list of results. In this setting our documents and query dataset would appear like this:\n\n\\(x_1 : q_1, (d_1, d_2)\\)\n\\(x_2 : q_2, (d_3, d_4, d_5)\\)\n\nFirst let‚Äôs look at the Permutation probability. Let‚Äôs denote \\(œÄ\\) as a specific permutation of a given list of length \\(n\\), \\(\\Theta (s_i) = f(q, d_i)\\) as any increasing function of scoring \\(s_i\\) given a query \\(q\\) and a document \\(i\\). The probability of having a permutation \\(œÄ\\) can be written as follows:\n\\[P(\\pi) = \\prod_{i=1}^n \\frac{\\phi(s_i)}{\\sum_{k=i}^n\\phi(s_k)}\\]\nTo illustrate consider a list of 3 items, then the probability of returning the permutation \\(s_1\\),\\(s_2\\),\\(s_3\\) is calculated as follows:\n\\[P(\\pi = \\{s_1, s_2, s_3\\}) = \\frac{\\phi(s_1)}{\\phi(s_1) + \\phi(s_2) + \\phi(s_3)} \\cdot \\frac{\\phi(s_2)}{\\phi(s_2) + \\phi(s_3)} \\cdot \\frac{\\phi(s_3)}{\\phi(s_3)}\\]\nDue to computational complexity, ListNet simplies the problem by looking at only the top-one probability of a given item. The top-one probability of object i equals the sum of the permutation probabilities of permutations in which object i is ranked on the top. Indeed, the top-one probability of object i can be written as:\n\\[P(i) = \\frac{\\phi(s_i)}{\\sum_{k=1}^n \\phi(s_k)}\\]\nGiven any two list of items represented by top-one probabilities, we can now measure the difference between them using cross entropy. Then we can use an ml algorithm which minimises that cross entropy. The choice of function \\(œï(‚ãÖ)\\), can be as simple as just an exponential function. When \\(œï(‚ãÖ)\\) is expotential and the list length is two, the solution basically reduces to a pairwise method as described earlier.\nWith that brief introduction out of the way, let‚Äôs also quickly look at the advantages and disadvantages of each of these approaches:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nPointwise\n\nSimplicity, Any standard ML models can be applied without any changes.\n\n\nThe results can be often sub-optimal due to not utilizing the full information of the entire list of matching documents for each query.\nThis requires explicit labels while constructing the dataset, and can be quite expensive.\n\n\n\nPairwise\n\nWe don‚Äôt need explicit labels, Only pairwise preferences are required.\nThe model learns how to rank directly, even though its a pairwise setting, but in theory it can approximate the performance of a general ranking task.\n\n\nThe Core Scoring mechanism is still pointwise. Hence, that relative information in the feature space among different documents given the same query is still not fully leveraged.\n\n\n\nListwise\n\nTheoretically this is a good solution for a ranking task.\n\n\nCostly to compute in its theoretical form and hence several approximations are used in practice.\nScoring function is still pointwise, which could be sub-optimal."
  },
  {
    "objectID": "posts/post-1/rank-with-moo-1.html#mean-average-precision-map",
    "href": "posts/post-1/rank-with-moo-1.html#mean-average-precision-map",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Mean Average Precision (MAP)",
    "text": "Mean Average Precision (MAP)\nMAP is a measure based on binary label of relevancy. To compute this first we define precision at k for a given query \\(P@k(q)\\) as:\n\\[P@k(q) \\equiv \\frac{\\sum_{i=1}^k r_i}{k}\\]\nfor an ordered list of prediction \\(r_i\\) for all \\(k\\) items. \\(ri=1\\) if it is relevant and 0 otherwise.Then we define the average precision given a query \\(AP(q)\\) at \\(k\\) items as:\n\\[AP(q)@k \\equiv \\frac{1}{\\sum_{i=1}^k r_i} \\sum_{i=1}^k P@i(q) \\times r_i\\]\nMean Average Precision is just the mean of \\(AP(q)\\) for all queries:\n\\[MAP \\equiv \\frac{\\sum_{q=1}^Q AP(q)}{Q}\\]\nAlso, MAP is an order sensitive metric because of the term \\(r_i\\) in the calculation of AP. It is essentially taking the average of precision at each ranking position and penalizing the precision at positions with irrelevant item by strcitly setting them to zeroes.\nHere is a simple example for computing MAP:"
  },
  {
    "objectID": "posts/post-1/rank-with-moo-1.html#mean-reciprocal-rank-mrr-expected-reciprocal-rank-err",
    "href": "posts/post-1/rank-with-moo-1.html#mean-reciprocal-rank-mrr-expected-reciprocal-rank-err",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Mean Reciprocal Rank (MRR) & Expected Reciprocal Rank (ERR)",
    "text": "Mean Reciprocal Rank (MRR) & Expected Reciprocal Rank (ERR)\nReciprocal rank metrics focus mainly on the first correctly predicted relevant item in a list. Given a list of items, and say \\(r_i\\) is the rank of the highest ranking relevant item & if the the 2nd item is the first relevant item in the list, then the reciprocal rank for this query would be \\(\\frac{1}{2}\\). By extension, each query will have a reciprocal rank. Hence, Mean reciprocal rank is essentially the average of reciprocal rank for all the queries, which would be represented as follows:\n\\[MRR \\equiv \\frac{1}{Q} \\sum_{i=1}^Q\\frac{1}{r_i}\\]\nExpected reciprocal rank tries to quantify how useful a document at rank \\(i\\) conditioned on the degree of relevance of documents at rank less than \\(i\\) are. The intution behind this is based on the empirical findings from web search task, that the likelihood a user will examine the document at rank \\(i\\) is dependent on how satisfied the user was with previously observed documents in the list.\nLets assume the probability of a user finding the result is satisfied at position \\(i\\) in a list of items is denoted as \\(R_i\\) & the likelihood of a session for which the user is satisfied and stops at position \\(r\\) is: \\[\\prod_{i=1}^{r-1}(1 - R_i)R_r\\]\nNow we can model \\(R_i\\) such that it is an increasing function of relevance:\n\\[R = R(g) \\equiv \\frac{2^g - 1}{2^{g_{max}}}\\]\nwhere \\(g\\) is the graded relevance such that \\(g \\in \\{0, 1, ..., g_{max}\\}\\) & \\(g = 0\\) implies an irrelevant document and \\(g = g_{max}\\) implies a relevant document.\nNow we can define ERR as follows:\n\\[ERR \\equiv \\sum_{r=1}^n\\frac{1}{r}R_r\\prod_{i=1}^{r-1}(1-R_i)\\]\nHere \\(\\frac{1}{r}\\) is treated as a utility function \\(\\tau(r)\\) that satisfies \\(\\tau(1) = 1\\) and \\(\\tau(r) \\rightarrow 0\\) as \\(r \\rightarrow \\infty\\).\nNote that ERR is a metric on a list with a single query, To evaluate results from multiple queries, we will need to further average ERRs among queries.\nHere is a simple example for computing MRR:"
  },
  {
    "objectID": "posts/post-1/rank-with-moo-1.html#normalized-discounted-cumulative-gain-ndcg",
    "href": "posts/post-1/rank-with-moo-1.html#normalized-discounted-cumulative-gain-ndcg",
    "title": "The Path towards Marketplace recommendations: Part-I",
    "section": "Normalized Discounted Cumulative Gain (NDCG)",
    "text": "Normalized Discounted Cumulative Gain (NDCG)\nNormalized Discounted Cumulative Gain (NDCG) is one of the most popular metric for measuring the quality of a set of ranked items in search or recommendations. If we were to break the assumptions made by this metric in simple terms, it would be as follows:\n\nCumulative Gain: Very relevant items are more useful than somewhat relevant items which are more useful than completely irrelevant items.\nDiscounting: Relevant items are more useful when they appear earlier in a list of ranked items.\nNormalization: The result of the ranking should be irrelevant to the query performed.\n\nLet‚Äôs define Discounted Cumulative Gain at position \\(k\\) as follows:\n\\[DCG@k \\equiv \\sum_{i=1}^k\\frac{2^{l_i} - 1}{log_2(i + 1)}\\]\nwhere \\(l_i\\) is the grading of relevance at rank \\(i\\). Intutively, the numerator is simply an increasing function of relevance, the more relevant the higher. This is the gain from each item. The denominator is a decreasing function of ranking position, this is the discounted component of the metric. Collectively, higher relevance gains more score, but the lower it is ranked the higher also the discount. Essentially, the metric will prefer higher relevant item to be ranked higher, which is the desired outcome.\nNDCG is then defined as:\n\\[NDCG@k = \\frac{DCG@k}{IDCG@k}\\]\nwhere \\(IDCG@k\\) is the Ideal DCG@k given the result. DCG@k is calculated by sorting the given list of items by its true relevance labels. and IDCG@k is the maximum possible DCG@K value one can get given a ranked list of items.\nHere is a simple example for computing NDCG:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "The Foundation: A Notes on Recsys, LTR, Ranking Evaluation metrics & Multi Objective Ranking in practice.\n\n\n\n\nltr\n\n\nrecsys\n\n\noptimisation\n\n\napplied-ml\n\n\nmarketplace\n\n\neconomics\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\nBharath G.S & Rita Anjana\n\n\n\n\n\n\nNo matching items"
  }
]